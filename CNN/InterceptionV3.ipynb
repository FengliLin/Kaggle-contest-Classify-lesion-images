{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from sklearn.svm import SVC\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(img_rows, img_cols, nb_channels) = (299, 299, 3)\n",
    "image_shape = (299, 299, 3)\n",
    "\n",
    "def read_data(path='../data/train.csv'):  \n",
    "    data_train = pd.read_csv(path)\n",
    "    X_data_train = data_train['ImageId']\n",
    "    y_data_train = data_train['Malignant']\n",
    "    X_train_id = X_data_train.values\n",
    "    Y_train = y_data_train.values\n",
    "\n",
    "    # input image dimensions\n",
    "    (img_rows, img_cols, nb_channels) = (299, 299, 3)\n",
    "    image_shape = (299, 299, 3)\n",
    "    data_size = X_train_id.shape[0]\n",
    "\n",
    "    # read data\n",
    "    X_image = np.zeros((data_size,img_rows, img_cols, nb_channels))\n",
    "    #X_image_mask = np.zeros((data_size,img_rows, img_cols, nb_channels))\n",
    "\n",
    "    for i in range(data_size):\n",
    "        name = X_train_id[i]\n",
    "        filename = '../data/im/{}.jpg'.format(name)\n",
    "        image = imread(filename)\n",
    "        #filename_Segmentation = 'data/im/{}_segmentation.jpg'.format(name)\n",
    "        #image_Segmentation = imread(filename_Segmentation) # Value 0 or 255\n",
    "        #image_Segmentation_boolean = (image_Segmentation/255).astype(np.uint8) # To get uint8\n",
    "        #image_Segmentation_expand = np.expand_dims(image_Segmentation_boolean, axis=2)\n",
    "        #image_mul_mask = (image_Segmentation_expand*image) \n",
    "        image = resize(image,image_shape, mode='reflect')\n",
    "        #image_mul_mask = resize(image_mul_mask,image_shape, mode='reflect')\n",
    "        X_image[i]= image\n",
    "        #X_image_mask[i] = image_mul_mask\n",
    "    #X_image_mask = np.moveaxis(X_image_mask, -1, 1)\n",
    "    return X_image.astype('float32')/255,Y_train[:data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "data,labels = read_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, shuffle= True)\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (630, 299, 299, 3)\n",
      "y_train shape: (630,)\n",
      "X_test shape: (70, 299, 299, 3)\n",
      "y_test shape: (70,)\n",
      "There is 630 train data\n",
      "There is 70 test data\n",
      "299 299 3\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('There is {} train data'.format(X_train.shape[0]))\n",
    "print('There is {} test data'.format(X_test.shape[0]))\n",
    "print(img_rows, img_cols, nb_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "validgen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "traingen.fit(X_train)\n",
    "validgen.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "40/39 [==============================] - 350s 9s/step - loss: 5.3731 - val_loss: 2.9315\n",
      "Epoch 2/10\n",
      "40/39 [==============================] - 337s 8s/step - loss: 0.9252 - val_loss: 1.0310\n",
      "Epoch 3/10\n",
      "40/39 [==============================] - 337s 8s/step - loss: 0.7510 - val_loss: 0.8076\n",
      "Epoch 4/10\n",
      "40/39 [==============================] - 336s 8s/step - loss: 0.6720 - val_loss: 0.7007\n",
      "Epoch 5/10\n",
      "40/39 [==============================] - 336s 8s/step - loss: 0.6270 - val_loss: 1.2669\n",
      "Epoch 6/10\n",
      "40/39 [==============================] - 335s 8s/step - loss: 0.6433 - val_loss: 0.7160\n",
      "Epoch 7/10\n",
      "40/39 [==============================] - 336s 8s/step - loss: 0.6685 - val_loss: 0.9636\n",
      "Epoch 8/10\n",
      "40/39 [==============================] - 336s 8s/step - loss: 0.6036 - val_loss: 0.8377\n",
      "Epoch 9/10\n",
      "40/39 [==============================] - 337s 8s/step - loss: 0.6349 - val_loss: 1.1341\n",
      "Epoch 10/10\n",
      "40/39 [==============================] - 336s 8s/step - loss: 0.5911 - val_loss: 1.1365\n",
      "0 input_3\n",
      "1 conv2d_189\n",
      "2 batch_normalization_189\n",
      "3 activation_189\n",
      "4 conv2d_190\n",
      "5 batch_normalization_190\n",
      "6 activation_190\n",
      "7 conv2d_191\n",
      "8 batch_normalization_191\n",
      "9 activation_191\n",
      "10 max_pooling2d_9\n",
      "11 conv2d_192\n",
      "12 batch_normalization_192\n",
      "13 activation_192\n",
      "14 conv2d_193\n",
      "15 batch_normalization_193\n",
      "16 activation_193\n",
      "17 max_pooling2d_10\n",
      "18 conv2d_197\n",
      "19 batch_normalization_197\n",
      "20 activation_197\n",
      "21 conv2d_195\n",
      "22 conv2d_198\n",
      "23 batch_normalization_195\n",
      "24 batch_normalization_198\n",
      "25 activation_195\n",
      "26 activation_198\n",
      "27 average_pooling2d_19\n",
      "28 conv2d_194\n",
      "29 conv2d_196\n",
      "30 conv2d_199\n",
      "31 conv2d_200\n",
      "32 batch_normalization_194\n",
      "33 batch_normalization_196\n",
      "34 batch_normalization_199\n",
      "35 batch_normalization_200\n",
      "36 activation_194\n",
      "37 activation_196\n",
      "38 activation_199\n",
      "39 activation_200\n",
      "40 mixed0\n",
      "41 conv2d_204\n",
      "42 batch_normalization_204\n",
      "43 activation_204\n",
      "44 conv2d_202\n",
      "45 conv2d_205\n",
      "46 batch_normalization_202\n",
      "47 batch_normalization_205\n",
      "48 activation_202\n",
      "49 activation_205\n",
      "50 average_pooling2d_20\n",
      "51 conv2d_201\n",
      "52 conv2d_203\n",
      "53 conv2d_206\n",
      "54 conv2d_207\n",
      "55 batch_normalization_201\n",
      "56 batch_normalization_203\n",
      "57 batch_normalization_206\n",
      "58 batch_normalization_207\n",
      "59 activation_201\n",
      "60 activation_203\n",
      "61 activation_206\n",
      "62 activation_207\n",
      "63 mixed1\n",
      "64 conv2d_211\n",
      "65 batch_normalization_211\n",
      "66 activation_211\n",
      "67 conv2d_209\n",
      "68 conv2d_212\n",
      "69 batch_normalization_209\n",
      "70 batch_normalization_212\n",
      "71 activation_209\n",
      "72 activation_212\n",
      "73 average_pooling2d_21\n",
      "74 conv2d_208\n",
      "75 conv2d_210\n",
      "76 conv2d_213\n",
      "77 conv2d_214\n",
      "78 batch_normalization_208\n",
      "79 batch_normalization_210\n",
      "80 batch_normalization_213\n",
      "81 batch_normalization_214\n",
      "82 activation_208\n",
      "83 activation_210\n",
      "84 activation_213\n",
      "85 activation_214\n",
      "86 mixed2\n",
      "87 conv2d_216\n",
      "88 batch_normalization_216\n",
      "89 activation_216\n",
      "90 conv2d_217\n",
      "91 batch_normalization_217\n",
      "92 activation_217\n",
      "93 conv2d_215\n",
      "94 conv2d_218\n",
      "95 batch_normalization_215\n",
      "96 batch_normalization_218\n",
      "97 activation_215\n",
      "98 activation_218\n",
      "99 max_pooling2d_11\n",
      "100 mixed3\n",
      "101 conv2d_223\n",
      "102 batch_normalization_223\n",
      "103 activation_223\n",
      "104 conv2d_224\n",
      "105 batch_normalization_224\n",
      "106 activation_224\n",
      "107 conv2d_220\n",
      "108 conv2d_225\n",
      "109 batch_normalization_220\n",
      "110 batch_normalization_225\n",
      "111 activation_220\n",
      "112 activation_225\n",
      "113 conv2d_221\n",
      "114 conv2d_226\n",
      "115 batch_normalization_221\n",
      "116 batch_normalization_226\n",
      "117 activation_221\n",
      "118 activation_226\n",
      "119 average_pooling2d_22\n",
      "120 conv2d_219\n",
      "121 conv2d_222\n",
      "122 conv2d_227\n",
      "123 conv2d_228\n",
      "124 batch_normalization_219\n",
      "125 batch_normalization_222\n",
      "126 batch_normalization_227\n",
      "127 batch_normalization_228\n",
      "128 activation_219\n",
      "129 activation_222\n",
      "130 activation_227\n",
      "131 activation_228\n",
      "132 mixed4\n",
      "133 conv2d_233\n",
      "134 batch_normalization_233\n",
      "135 activation_233\n",
      "136 conv2d_234\n",
      "137 batch_normalization_234\n",
      "138 activation_234\n",
      "139 conv2d_230\n",
      "140 conv2d_235\n",
      "141 batch_normalization_230\n",
      "142 batch_normalization_235\n",
      "143 activation_230\n",
      "144 activation_235\n",
      "145 conv2d_231\n",
      "146 conv2d_236\n",
      "147 batch_normalization_231\n",
      "148 batch_normalization_236\n",
      "149 activation_231\n",
      "150 activation_236\n",
      "151 average_pooling2d_23\n",
      "152 conv2d_229\n",
      "153 conv2d_232\n",
      "154 conv2d_237\n",
      "155 conv2d_238\n",
      "156 batch_normalization_229\n",
      "157 batch_normalization_232\n",
      "158 batch_normalization_237\n",
      "159 batch_normalization_238\n",
      "160 activation_229\n",
      "161 activation_232\n",
      "162 activation_237\n",
      "163 activation_238\n",
      "164 mixed5\n",
      "165 conv2d_243\n",
      "166 batch_normalization_243\n",
      "167 activation_243\n",
      "168 conv2d_244\n",
      "169 batch_normalization_244\n",
      "170 activation_244\n",
      "171 conv2d_240\n",
      "172 conv2d_245\n",
      "173 batch_normalization_240\n",
      "174 batch_normalization_245\n",
      "175 activation_240\n",
      "176 activation_245\n",
      "177 conv2d_241\n",
      "178 conv2d_246\n",
      "179 batch_normalization_241\n",
      "180 batch_normalization_246\n",
      "181 activation_241\n",
      "182 activation_246\n",
      "183 average_pooling2d_24\n",
      "184 conv2d_239\n",
      "185 conv2d_242\n",
      "186 conv2d_247\n",
      "187 conv2d_248\n",
      "188 batch_normalization_239\n",
      "189 batch_normalization_242\n",
      "190 batch_normalization_247\n",
      "191 batch_normalization_248\n",
      "192 activation_239\n",
      "193 activation_242\n",
      "194 activation_247\n",
      "195 activation_248\n",
      "196 mixed6\n",
      "197 conv2d_253\n",
      "198 batch_normalization_253\n",
      "199 activation_253\n",
      "200 conv2d_254\n",
      "201 batch_normalization_254\n",
      "202 activation_254\n",
      "203 conv2d_250\n",
      "204 conv2d_255\n",
      "205 batch_normalization_250\n",
      "206 batch_normalization_255\n",
      "207 activation_250\n",
      "208 activation_255\n",
      "209 conv2d_251\n",
      "210 conv2d_256\n",
      "211 batch_normalization_251\n",
      "212 batch_normalization_256\n",
      "213 activation_251\n",
      "214 activation_256\n",
      "215 average_pooling2d_25\n",
      "216 conv2d_249\n",
      "217 conv2d_252\n",
      "218 conv2d_257\n",
      "219 conv2d_258\n",
      "220 batch_normalization_249\n",
      "221 batch_normalization_252\n",
      "222 batch_normalization_257\n",
      "223 batch_normalization_258\n",
      "224 activation_249\n",
      "225 activation_252\n",
      "226 activation_257\n",
      "227 activation_258\n",
      "228 mixed7\n",
      "229 conv2d_261\n",
      "230 batch_normalization_261\n",
      "231 activation_261\n",
      "232 conv2d_262\n",
      "233 batch_normalization_262\n",
      "234 activation_262\n",
      "235 conv2d_259\n",
      "236 conv2d_263\n",
      "237 batch_normalization_259\n",
      "238 batch_normalization_263\n",
      "239 activation_259\n",
      "240 activation_263\n",
      "241 conv2d_260\n",
      "242 conv2d_264\n",
      "243 batch_normalization_260\n",
      "244 batch_normalization_264\n",
      "245 activation_260\n",
      "246 activation_264\n",
      "247 max_pooling2d_12\n",
      "248 mixed8\n",
      "249 conv2d_269\n",
      "250 batch_normalization_269\n",
      "251 activation_269\n",
      "252 conv2d_266\n",
      "253 conv2d_270\n",
      "254 batch_normalization_266\n",
      "255 batch_normalization_270\n",
      "256 activation_266\n",
      "257 activation_270\n",
      "258 conv2d_267\n",
      "259 conv2d_268\n",
      "260 conv2d_271\n",
      "261 conv2d_272\n",
      "262 average_pooling2d_26\n",
      "263 conv2d_265\n",
      "264 batch_normalization_267\n",
      "265 batch_normalization_268\n",
      "266 batch_normalization_271\n",
      "267 batch_normalization_272\n",
      "268 conv2d_273\n",
      "269 batch_normalization_265\n",
      "270 activation_267\n",
      "271 activation_268\n",
      "272 activation_271\n",
      "273 activation_272\n",
      "274 batch_normalization_273\n",
      "275 activation_265\n",
      "276 mixed9_0\n",
      "277 concatenate_5\n",
      "278 activation_273\n",
      "279 mixed9\n",
      "280 conv2d_278\n",
      "281 batch_normalization_278\n",
      "282 activation_278\n",
      "283 conv2d_275\n",
      "284 conv2d_279\n",
      "285 batch_normalization_275\n",
      "286 batch_normalization_279\n",
      "287 activation_275\n",
      "288 activation_279\n",
      "289 conv2d_276\n",
      "290 conv2d_277\n",
      "291 conv2d_280\n",
      "292 conv2d_281\n",
      "293 average_pooling2d_27\n",
      "294 conv2d_274\n",
      "295 batch_normalization_276\n",
      "296 batch_normalization_277\n",
      "297 batch_normalization_280\n",
      "298 batch_normalization_281\n",
      "299 conv2d_282\n",
      "300 batch_normalization_274\n",
      "301 activation_276\n",
      "302 activation_277\n",
      "303 activation_280\n",
      "304 activation_281\n",
      "305 batch_normalization_282\n",
      "306 activation_274\n",
      "307 mixed9_1\n",
      "308 concatenate_6\n",
      "309 activation_282\n",
      "310 mixed10\n",
      "Epoch 1/20\n",
      "40/39 [==============================] - 421s 11s/step - loss: 0.7327 - val_loss: 0.6945\n",
      "Epoch 2/20\n",
      "40/39 [==============================] - 409s 10s/step - loss: 0.5253 - val_loss: 0.6792\n",
      "Epoch 3/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.5239 - val_loss: 0.6976\n",
      "Epoch 4/20\n",
      "40/39 [==============================] - 412s 10s/step - loss: 0.5036 - val_loss: 0.6896\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/39 [==============================] - 411s 10s/step - loss: 0.4972 - val_loss: 0.7134\n",
      "Epoch 6/20\n",
      "40/39 [==============================] - 413s 10s/step - loss: 0.5094 - val_loss: 0.7054\n",
      "Epoch 7/20\n",
      "40/39 [==============================] - 411s 10s/step - loss: 0.4892 - val_loss: 0.6765\n",
      "Epoch 8/20\n",
      "40/39 [==============================] - 410s 10s/step - loss: 0.4989 - val_loss: 0.6995\n",
      "Epoch 9/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.4645 - val_loss: 0.7001\n",
      "Epoch 10/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.4752 - val_loss: 0.7023\n",
      "Epoch 11/20\n",
      "40/39 [==============================] - 409s 10s/step - loss: 0.4847 - val_loss: 0.7475\n",
      "Epoch 12/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.4768 - val_loss: 0.6726\n",
      "Epoch 13/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.4647 - val_loss: 0.7327\n",
      "Epoch 14/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.4686 - val_loss: 0.7383\n",
      "Epoch 15/20\n",
      "40/39 [==============================] - 410s 10s/step - loss: 0.4807 - val_loss: 0.7066\n",
      "Epoch 16/20\n",
      "40/39 [==============================] - 409s 10s/step - loss: 0.4448 - val_loss: 0.7254\n",
      "Epoch 17/20\n",
      "40/39 [==============================] - 410s 10s/step - loss: 0.4528 - val_loss: 0.7665\n",
      "Epoch 18/20\n",
      "40/39 [==============================] - 410s 10s/step - loss: 0.4393 - val_loss: 0.7748\n",
      "Epoch 19/20\n",
      "40/39 [==============================] - 410s 10s/step - loss: 0.4491 - val_loss: 0.7202\n",
      "Epoch 20/20\n",
      "40/39 [==============================] - 408s 10s/step - loss: 0.4593 - val_loss: 0.6953\n"
     ]
    }
   ],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# and a logistic layer -- let's say we have 2 classes\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(traingen.flow(X_train, y_train, batch_size=16),\n",
    "      steps_per_epoch=len(X_train) / 16 ,\n",
    "      epochs=10,\n",
    "      validation_data=validgen.flow(X_test, y_test, batch_size=16),\n",
    "      validation_steps=len(X_test) / 16,\n",
    "      class_weight=class_weights,\n",
    "      verbose=1)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(traingen.flow(X_train, y_train, batch_size=16),\n",
    "      steps_per_epoch=len(X_train) / 16 ,\n",
    "      epochs=20,\n",
    "      validation_data=validgen.flow(X_test, y_test, batch_size=16),\n",
    "      validation_steps=len(X_test) / 16,\n",
    "      class_weight=class_weights,\n",
    "      verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save('InterceptionV3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
